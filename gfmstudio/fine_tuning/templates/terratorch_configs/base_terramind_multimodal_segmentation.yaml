# Â© Copyright IBM Corporation 2025
# SPDX-License-Identifier: Apache-2.0


# lightning.pytorch==2.1.1
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 16-mixed
  logger:
    class_path: lightning.pytorch.loggers.mlflow.MLFlowLogger
    init_args:
      experiment_name: {{ tune_id }} 
      run_name: "Train"    
      tracking_uri: {{ mlflow_tracking_url }}
      save_dir: {{ mount_root + 'tune-tasks/' + tune_id + '/mlflow' }}
      {% if mlflow_tags -%}
      tags:
        {% for key, value in mlflow_tags.items() -%}
        {{ key }}: {{ value }}
        {% endfor %}
      {%- endif %}       
  callbacks:
    - class_path: RichProgressBar
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch
    # ---- Early stop if ----
    {% if runner["early_stopping_patience"] -%}
    - class_path: EarlyStopping
      init_args:
        monitor: {{ runner["early_stopping_monitor"] }}
        patience: {{ runner["early_stopping_patience"] }}
    {%- endif %}
     # ---- Early stop endif ----
    - class_path: ModelCheckpoint
      init_args:
        dirpath: {{ mount_root + 'tune-tasks/' + tune_id  + '/' }}
        mode: min
        monitor: val/loss
        filename: {{ 'best-state_dict-{epoch:02d}' }}
        save_weights_only: True
  max_epochs: {{ runner["max_epochs"] }}
  check_val_every_n_epoch: {{ evaluation["interval"] }}
  log_every_n_steps: 50
  enable_checkpointing: true
  default_root_dir: {{ mount_root + 'tune-tasks/' + tune_id }}
data:
  {% if image_modalities|length == 1 %} 
  class_path: GenericNonGeoSegmentationDataModule
  init_args:
    batch_size: 2
    num_workers: 1
    dataset_bands:  # Dataset bands
      {{ bands.values() | list | first | to_yaml | indent(6) }}
    rgb_indices:
      {{ rgb_band_indices | to_yaml | indent(6) }}
    train_data_root: {{ data_root }}{{ train_data_dir.values() | list | first }}
    val_data_root: {{ data_root }}{{ val_data_dir.values() | list | first }}
    test_data_root: {{ data_root }}{{ test_data_dir.values() | list | first }}
    # labels roots
    train_label_data_root: {{ data_root + train_labels_dir }}
    val_label_data_root: {{ data_root + val_labels_dir }}
    test_label_data_root: {{ data_root + test_labels_dir }}
    {% if train_split_path -%}
    train_split: {{ data_root + train_split_path }}
    {% endif -%}
    {% if test_split_path -%}
    test_split: {{ data_root + test_split_path }}
    {% endif -%}
    {% if val_split_path -%}
    val_split: {{ data_root + val_split_path }}
    {% endif -%}

    {% if img_suffix -%}
    img_grep:  {{ img_suffix.values() | list | first | tojson }}
    {% endif -%}
    {% if seg_map_suffix -%}
    label_grep: "{{ seg_map_suffix }}"
    {% endif -%}
    means: 
      {{ norm_means.values() | list | first| to_yaml | indent(6) }}
    stds: 
      {{ norm_stds.values() | list | first | to_yaml | indent(6) }}
    num_classes: {{ classes|length }}
    train_transform:
      - class_path: albumentations.D4
      - class_path: ToTensorV2
    no_data_replace: 0
    no_label_replace: -1


  {% else %}
  class_path: terratorch.datamodules.GenericMultiModalDataModule
  init_args:
    # Config for only segmentation. No need to automate this. 
    task: 'segmentation'
    # Out of cuda error for anything > 2
    # ToDo: Figure out why batch_size replacement is not working.
    batch_size: 2
    {% if num_workers -%}
    num_workers: {{ num_workers }}
    {% else -%}
    num_workers: 2
    {% endif -%}
    no_label_replace: {{ label_nodata }}
    no_data_replace: {{ image_nodata_replace }}
    dataset_bands:
      {{ bands | to_yaml | indent(6)}}
    output_bands:
      {{ output_bands | to_yaml  | indent(6)}}
    modalities:
      {{ image_modalities | to_yaml | indent(6) }}
    rgb_modality: {{ rgb_modality }}
    rgb_indices: 
      {{ rgb_band_indices | to_yaml | indent(6) }}
    train_data_root:
      {% for key, val in train_data_dir.items() -%}
       {{ key }}: {{ data_root }}{{ val }}
      {% endfor %}
    train_label_data_root:  {{ data_root +  train_labels_dir }}
    val_data_root:
      {% for key, val in val_data_dir.items() -%}
       {{ key }}: {{ data_root }}{{ val }}
      {% endfor %}
    val_label_data_root: {{ data_root +  test_labels_dir }}
    test_data_root:
      {% for key, val in test_data_dir.items() -%}
       {{ key }}: {{ data_root }}{{ val }}
      {% endfor %}
    test_label_data_root: {{ data_root + test_labels_dir }}
    {% if train_split_path -%}
    train_split: {{  data_root + train_split_path }}
    {% endif -%}
    {% if test_split_path -%}
    test_split: {{  data_root + test_split_path }}
    {% endif -%}
    {% if val_split_path -%}
    val_split: {{  data_root + val_split_path }}
    {% endif -%}
    {% if img_suffix -%}
    image_grep: 
      {{ img_suffix | to_yaml | indent(6) }}
    {% endif -%}
    {% if seg_map_suffix -%}
    label_grep: "{{ seg_map_suffix }}"
    {% endif -%}

    num_classes: {{ classes|length }}
    {% if data["expand_temporal_dimension"] is not none -%}
    expand_temporal_dimension: data["expand_temporal_dimension"]
    {% endif -%}
    {% if data["drop_last"] is not none -%}
    drop_last: data["drop_last"]
    {% endif -%}

    means: 
      {{ norm_means | to_yaml | indent(6) }}
    stds: 
      {{ norm_stds | to_yaml | indent(6) }}
    train_transform:
      - class_path: albumentations.D4  # Random flip and rotations
      - class_path: ToTensorV2
  {% endif %}
model:
  class_path: terratorch.tasks.SemanticSegmentationTask
  init_args:
    model_factory: EncoderDecoderFactory
    model_args:
      backbone: {{ pretrained_model_name }}
      #  terramind_v1_base  # large version: terramind_v1_large
      backbone_pretrained: true
      {%- if image_modalities %}
      backbone_modalities:
        {{ image_modalities | to_yaml  | indent(8)}}
      {%- endif %}
      {% if  image_modalities|length > 1 %}
      backbone_merge_method: mean
      {% endif %}
      backbone_bands: 
        {{ output_bands | to_yaml | indent(8) }}
      necks:
        {%- if pretrained_model_name == "terramind_v1_base" %}
        - name: SelectIndices
          indices: [2, 5, 8, 11]  # base version
        {%- elif pretrained_model_name == "terramind_v1_large" %}
        - name: SelectIndices
          indices: [5, 11, 17, 23]  # large version
        {% endif %}
        - name: ReshapeTokensToImage
          remove_cls_token: False
        - name: LearnedInterpolateToPyramidal

      decoder: {{ model["decode_head"]["decoder"] }}
      # UNetDecoder
      {% if  model["decode_head"]["decoder"] == "UperNetDecoder" -%}
      decoder_channels: [512, 256, 128, 64]
      {% elif  model["decode_head"]["decoder"] == "UNetDecoder" -%}
      #TODO user provided channels
      decoder_channels: [512, 256, 128, 64]
      {% else %}
      decoder_channels: {{ model["decode_head"]["channels"] }}
      {% endif -%}
      head_dropout: 0.1
      num_classes: {{ classes|length }}
    loss: {{ model["decode_head"]["loss_decode"]["type"] }}
    plot_on_val: {{ runner["plot_on_val"] }}
    #  dice
    ignore_index: {{ ignore_index }}
    freeze_backbone:  {{ model["frozen_backbone"] | lower }}
    freeze_decoder: false
    {% if  model["class_names"] %}
    class_names: {{ model["class_names"] | to_yaml | indent(6)}}
    {% endif %}
    # ---- optimizer start ----
    {% if model["optimizer"] -%}
    optimizer: {{ model["optimizer"]["type"] }}
    lr: {{ model["optimizer"]["lr"] | float }}
    {% endif -%}
    # ---- optimizer end ----
    {% if model["tiled_inference_parameters"] %}
    tiled_inference_parameters: 
      h_crop: {{ model["tiled_inference_parameters"]["h_crop"] | int}}
      h_stride: {{ model["tiled_inference_parameters"]["h_stride"] | int }}
      w_crop: {{ model["tiled_inference_parameters"]["w_crop"] | int}}
      w_stride: {{ model["tiled_inference_parameters"]["w_stride"] | int }}
      average_patches: {{ model["tiled_inference_parameters"]["average_patches"] }}
    {% else %}
    # ToDo: Remove the tiled_inference if user not provided. 
    tiled_inference_parameters: 
      h_crop: 512
      # stride logic = would be h_crop - h_crop * 0.125
      h_stride: 448
      w_crop: 512
      # stride logic = would be w_crop - w_crop * 0.125
      w_stride: 448
      average_patches: true
    {% endif %}

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    {% if optimizer["lr"] -%}
    lr: {{ optimizer["lr"] | float }}
    {% else %}
    lr: 2.e-5
    {% endif -%}
    {%- if optimizer["weight_decay"] -%}
    weight_decay: {{ optimizer["weight_decay"] }}
    {%- else -%}
    weight_decay: 0.05
    {% endif %}
lr_scheduler:
  class_path: ReduceLROnPlateau
  init_args:
    monitor: val/loss
    factor: 0.5
    patience: 5
